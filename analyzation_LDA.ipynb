{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "from string import digits\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your html file in this folder\n",
    "\n",
    "html_files = os.listdir('./web_datasets/')\n",
    "files_number = len(html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of a file cleaning\n",
    "\n",
    "def cleanHTMLDocument(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\") \n",
    "    for script in soup([\"script\", \"style\"]): \n",
    "        script.extract()\n",
    "\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.857234001159668 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Read necessay files\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "files = [0] * files_number \n",
    "df = pd.DataFrame(data={}, columns = ['text'])\n",
    "\n",
    "for i in range(files_number):\n",
    "    with open(\"./web_datasets/\" + html_files[i], \"r\", encoding='utf-8') as f:\n",
    "        try:\n",
    "            text = cleanHTMLDocument(f.read())\n",
    "            text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "            df = df.append({'text': text}, ignore_index=True)   \n",
    "            \n",
    "        except:\n",
    "            files[i] = \" \"\n",
    "            print(\"problem with file\" + html_files[i])\n",
    "            \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional stop words\n",
    "\n",
    "other_stop_words = [\n",
    "    'contact', 'cookies', 'confirm', 'website', 'share', 'login', 'password',\n",
    "    'course', 'learn', 'programming', 'lpa', 'code', 'courses', 'java', 'learning', 'get', 'one', 'python', 'see', 'pluralsight', 'web',\n",
    "    'org', 'become', 'javascript', 'android', 'may', 'site', 'developer'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/beon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Clean datasets\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(other_stop_words)\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def delete_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "data = df['text'].values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "data_words = delete_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.009*\"students\" + 0.009*\"read\" + 0.007*\"team\" + 0.007*\"information\" + 0.006*\"review\" + 0.006*\"free\" + 0.006*\"data\" + 0.005*\"full\" + 0.005*\"hours\" + 0.005*\"weekgraduate\"'), (1, '0.011*\"students\" + 0.009*\"team\" + 0.008*\"read\" + 0.008*\"review\" + 0.007*\"free\" + 0.005*\"reviews\" + 0.005*\"projects\" + 0.005*\"step\" + 0.004*\"way\" + 0.004*\"build\"'), (2, '0.007*\"free\" + 0.007*\"read\" + 0.005*\"path\" + 0.005*\"students\" + 0.005*\"data\" + 0.005*\"career\" + 0.004*\"team\" + 0.004*\"review\" + 0.004*\"skills\" + 0.004*\"experience\"'), (3, '0.012*\"students\" + 0.011*\"review\" + 0.010*\"team\" + 0.009*\"read\" + 0.007*\"free\" + 0.006*\"skills\" + 0.004*\"reviews\" + 0.004*\"month\" + 0.004*\"using\" + 0.004*\"way\"'), (4, '0.008*\"team\" + 0.008*\"skills\" + 0.007*\"students\" + 0.006*\"read\" + 0.006*\"data\" + 0.005*\"free\" + 0.005*\"review\" + 0.005*\"skill\" + 0.005*\"information\" + 0.004*\"hours\"'), (5, '0.009*\"review\" + 0.009*\"students\" + 0.008*\"read\" + 0.008*\"free\" + 0.007*\"team\" + 0.005*\"projects\" + 0.005*\"build\" + 0.004*\"reviews\" + 0.004*\"privacy\" + 0.004*\"skills\"'), (6, '0.018*\"students\" + 0.016*\"read\" + 0.016*\"team\" + 0.012*\"review\" + 0.011*\"free\" + 0.008*\"reviews\" + 0.005*\"skills\" + 0.005*\"data\" + 0.004*\"experience\" + 0.004*\"time\"'), (7, '0.008*\"read\" + 0.008*\"team\" + 0.007*\"review\" + 0.006*\"skills\" + 0.006*\"students\" + 0.005*\"free\" + 0.004*\"month\" + 0.003*\"reviews\" + 0.003*\"data\" + 0.003*\"way\"'), (8, '0.017*\"students\" + 0.015*\"review\" + 0.014*\"read\" + 0.012*\"team\" + 0.011*\"reviews\" + 0.010*\"free\" + 0.005*\"masterclass\" + 0.005*\"skills\" + 0.004*\"instructor\" + 0.004*\"month\"'), (9, '0.014*\"read\" + 0.014*\"review\" + 0.013*\"students\" + 0.013*\"team\" + 0.007*\"free\" + 0.006*\"reviews\" + 0.006*\"information\" + 0.006*\"data\" + 0.005*\"skills\" + 0.004*\"experience\"')]\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "\n",
    "id_2_word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "corpus = [id_2_word.doc2bow(text) for text in texts]\n",
    "\n",
    "number_topics = 10\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id_2_word,\n",
    "                                       num_topics=number_topics)\n",
    "\n",
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word counter\n",
    "\n",
    "def freq(data_words):\n",
    "    df = pd.DataFrame({}, columns=['Word', 'Frequency'])\n",
    "    \n",
    "    unique_words = set(data_words)\n",
    "      \n",
    "    for word in unique_words:\n",
    "        df = df.append({'Word' : word,\n",
    "                'Frequency' : data_words.count(word)}, \n",
    "                ignore_index=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "data_words = itertools.chain.from_iterable(data_words)\n",
    "data_words = list(data_words)\n",
    "frequency = freq(data_words)\n",
    "frequency = frequency.sort_values(by=\"Frequency\", ascending=False)\n",
    "frequency.to_csv('frequency.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
